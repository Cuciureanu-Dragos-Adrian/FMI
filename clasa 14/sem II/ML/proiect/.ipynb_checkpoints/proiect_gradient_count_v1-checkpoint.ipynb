{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca943ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c1e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data\n",
    "def load_sample(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    \n",
    "    indexes = []\n",
    "    sentences = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        indexes.append(int(\"\".join(line[:6])))\n",
    "        sentences.append(line[7:].strip('\\n'))\n",
    "        \n",
    "    return indexes, sentences\n",
    "\n",
    "\n",
    "def load_label(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        sentences.append(int(line[7]))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f3056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_indexes, train_samples = load_sample(\"data/train_samples.txt\")\n",
    "train_labels = load_label(\"data/train_labels.txt\")\n",
    "\n",
    "#validation data\n",
    "validation_indexes, validation_samples = load_sample(\"data/validation_samples.txt\")\n",
    "validation_labels = load_label(\"data/validation_labels.txt\")\n",
    "\n",
    "#test data\n",
    "test_indexes, test_samples = load_sample(\"data/test_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51377950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 50 0.6042\n",
      "0.1 100 0.6352\n",
      "0.1 150 0.6476\n"
     ]
    }
   ],
   "source": [
    "#initialize the gradient boosting model\n",
    "gradient_model = GradientBoostingClassifier()\n",
    "\n",
    "#initialize the count vector for our words\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "vocabulary = train_samples + test_samples\n",
    "count_vector.fit(vocabulary)\n",
    "\n",
    "train_samples_count = count_vector.transform(train_samples)\n",
    "validation_samples_count = count_vector.transform(validation_samples)\n",
    "test_samples_count = count_vector.transform(test_samples)\n",
    "\n",
    "learning_rate = [0.1]\n",
    "estimators = [50, 100, 150]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    for est in estimators:\n",
    "        gradient_model = GradientBoostingClassifier(n_estimators=est, learning_rate=lr)\n",
    "        gradient_model.fit(train_samples_count, train_labels)\n",
    "\n",
    "        predicted = gradient_model.predict(validation_samples_count)\n",
    "\n",
    "        print(lr, est, accuracy_score(predicted, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f83946c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Uq$%y', 'gkuKDuZ*KmH', 'quf&', 'qKf', 'Du*&', 'Du*ZX;', 'Kf', 'DY*fHm', 'ZuK&', 'XZ*KTYDX&kK', 'K;YuE', '*u', 'AKmH', 'KEHqqH&', '*u', 'ZK', ',fKq', 'K;', 'qXZYZ*Huh', 'qK;*HDE', 'K;', 'DKZf*AXD', 'p', 'Z**m', 'Yuh', 'qDuK*YT', 'qK;', 'KEHTY’f', 'U', ',ènSè', 'Hf', 'q*H;', 'KDmYmDuq', 'K;', 'Kf#Yqq*&', 'qY*sKZu*Z', 'qséȘ;', 'ZqK', 'fI', 'gkuKDKE*H;', 'Z*KTKDCYfuAYZDH&', 'qsYZAH', 'q*HAf*m', 'qK;', 'ZK', ',qKuhYk*Z', 'NHE', 'qK;', ',Z*HffYu*#', 'KuhYDusfuq', 'K;YAH’;', 'qKuhqHm', 'qK;', 'Kmu*DZ', 'p', '**', ',ènSè', '°', '85', 'KD;*YKZZH', 'Z*KmuK&', 'Yuh', 'qKDuZHDX&TKZ', 'qK;', 'quf&', '*n', 'gqKTRDZkK', 'q**YZY;**A', 'qK;', 'ZY*uXD', 'ènSè', 'Kf', ',»', 'ZD*T', 'Hf', 'K;', 'KDY*ZYDDKZ', '«', 'XqYZ&H#', 'H', 'ènSè', 'Yuh', 'ènSè', 'ènSè', 'DuKZHD*f&kK', 'ènSè', 'DH&', '0391', 'qKX**H', 'qKf', 'q*H;', 'ènSè']\n"
     ]
    }
   ],
   "source": [
    "#initialize the count vector for our words\n",
    "count_vector = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "vocabulary = train_samples + validation_samples + test_samples\n",
    "print(vocabulary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf7bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector.fit(vocabulary)\n",
    "\n",
    "all_train_samples = train_samples + validation_samples\n",
    "all_train_samples_count = count_vector.transform(all_train_samples)\n",
    "\n",
    "gradient_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.5)\n",
    "gradient_model.fit(all_train_samples_count, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the prediction on the test data\n",
    "predicted = gradient_model.predict(test_samples_count)\n",
    "\n",
    "#and write it in the csv\n",
    "g = open(\"data/test_labels.txt\", 'w')\n",
    "g.write(\"id,label\\n\")\n",
    "\n",
    "for idx in range(len(predicted)):\n",
    "    g.write(f\"{test_indexes[idx]},{predicted[idx]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
