{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020d4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the modules we need\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "284aae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the normalize function\n",
    "def normalize_data(data, type1=None):\n",
    "    scaler = None\n",
    "\n",
    "    if type1 == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(data)\n",
    "        \n",
    "        scaled_data = scaler.transform(data)\n",
    "        return scaled_data\n",
    "    \n",
    "    elif type1 == 'l1':\n",
    "        scaler = preprocessing.Normalizer(norm='l1')\n",
    "        scaler.fit(data)\n",
    "        \n",
    "        scaled_data = scaler.transform(data)\n",
    "        return scaled_data\n",
    "            \n",
    "    elif type1 == 'l2':\n",
    "        scaler = preprocessing.Normalizer(norm='l2')\n",
    "        scaler.fit(data)\n",
    "        \n",
    "        scaled_data = scaler.transform(data)\n",
    "        return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6903b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words class, made during ml lab\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = dict()\n",
    "        self.words = []  \n",
    "        \n",
    "        \n",
    "    def build_vocabulary(self, data):\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary[word] = len(self.vocabulary)\n",
    "                    self.words.append(word)\n",
    "                    \n",
    "            \n",
    "    def get_features(self, data):\n",
    "        features = np.zeros((len(data), len(self.vocabulary)))\n",
    "        \n",
    "        for id_sen, document in enumerate(data):\n",
    "            for word in document:\n",
    "                if word in self.vocabulary:\n",
    "                    features[id_sen, self.vocabulary[word]] += 1\n",
    "                    \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d6b84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data\n",
    "def load_sample(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    \n",
    "    indexes = []\n",
    "    sentences = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        indexes.append(int(\"\".join(line[:6])))\n",
    "        sentences.append(line[7:].strip('\\n').split())\n",
    "        \n",
    "    return indexes, sentences\n",
    "\n",
    "\n",
    "def load_label(file_name):\n",
    "    f = open(file_name, 'r', encoding='utf8')\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        sentences.append(int(line[7]))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59e5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "train_indexes, train_samples = load_sample(\"data/train_samples.txt\")\n",
    "train_labels = load_label(\"data/train_labels.txt\")\n",
    "\n",
    "#validation data\n",
    "validation_indexes, validation_samples = load_sample(\"data/validation_samples.txt\")\n",
    "validation_labels = load_label(\"data/validation_labels.txt\")\n",
    "\n",
    "#test data\n",
    "test_indexes, test_samples = load_sample(\"data/test_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2542b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize our vocabulary and get the features\n",
    "bow = BagOfWords()\n",
    "bow.build_vocabulary(train_samples)\n",
    "\n",
    "train_features = bow.get_features(train_samples)\n",
    "validation_features = bow.get_features(validation_samples)\n",
    "test_features = bow.get_features(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dd14b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize our features\n",
    "train_features_stand = normalize_data(train_features, type1 = 'l2')\n",
    "validation_features_stand = normalize_data(validation_features, type1 = 'l2')\n",
    "test_features_stand = normalize_data(test_features, type1 = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f228d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "decision_tree_model.fit(train_features_stand, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0061ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5896\n"
     ]
    }
   ],
   "source": [
    "#get the prediction on the validation data\n",
    "predicted = decision_tree_model.predict(validation_features_stand)\n",
    "\n",
    "print(np.mean(predicted == validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b0c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_samples = train_samples + validation_samples\n",
    "all_train_labels = train_labels + validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "437f148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_final = BagOfWords()\n",
    "bow_final.build_vocabulary(all_train_samples)\n",
    "\n",
    "all_train_features = bow_final.get_features(all_train_samples)\n",
    "test_features = bow_final.get_features(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eab3e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "decision_tree_model_final = DecisionTreeClassifier()\n",
    "decision_tree_model_final.fit(all_train_features, all_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c1e0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the prediction on the validation data\n",
    "predicted_final = decision_tree_model_final.predict(test_features)\n",
    "\n",
    "#and write it in the csv\n",
    "g = open(\"data/test_labels.txt\", 'w')\n",
    "g.write(\"id,label\\n\")\n",
    "\n",
    "for idx in range(len(predicted)):\n",
    "    g.write(f\"{test_indexes[idx]},{predicted[idx]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed360fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
